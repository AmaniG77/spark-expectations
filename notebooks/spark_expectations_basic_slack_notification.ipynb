{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac24247",
   "metadata": {},
   "source": [
    "<div style=\"color:#fff; background:#0070c0; font-weight:bold; border:2px solid #0070c0; padding:10px; border-radius:6px;\">\n",
    "**Purpose:** This notebook demonstrates how to use <span style=\"color:yellow;\">Spark Expectations</span> with <span style=\"color:yellow;\"> Slack by sending notifications`</span>.<br>\n",
    "Its main focus is to show how data quality alerts and results can be sent to Slack channels.</span>\n",
    "</div>\n",
    "\n",
    "### Spark - Expectations - User - Guide - Documentation\n",
    "\n",
    "<div style=\"color:red; font-weight:bold; border:2px solid red; padding:8px;\">\n",
    "⚠️ ALERT: Notebook meant to be ran by spinning up local docker compose (containers/compose.yaml) !\n",
    "</div>\n",
    "\n",
    "* Please read through the [Spark Expectation Documentation](https://engineering.nike.com/spark-expectations) before proceeding with this demo\n",
    "\n",
    "#### widgets \n",
    "* `catalog`, `schema` - leave default values \n",
    "  * Tables are going to be prefixed with value provided in user widget text field\n",
    "\n",
    "<div style=\"color:orange; font-weight:regular; border:1px solid orange; padding:8px;\">\n",
    "⚠️ Container comes with SparkExpectation by default. If SE version is overriden Kernel will need to be restarted!\n",
    "</div>\n",
    "\n",
    "* `Override SE version` check box to install different SparkExpectation library version\n",
    "* `library_source` combo box defines library url(git branch or pypi) from where to pull library \n",
    "  * `pypi` ( installs latest published version available in PyPi)\n",
    "  * `git` ( installs library from specified git branch)\n",
    "    * Set `git_branch` input field to match git branch (example `main`)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e8e62",
   "metadata": {},
   "source": [
    "# Method 1: Use Webhook URL for Notebook Testing\n",
    "\n",
    "This is used by default for the example notebook. This will just use the webhook URL throughout the notebook for a quick and easy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b024da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local Spark environment - using webhook URL directly\n",
      "Webhook URL configured: https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
     ]
    }
   ],
   "source": [
    "# For local Spark environment, we'll use webhook URL directly\n",
    "# This section would be used in Databricks to create secret scopes\n",
    "\n",
    "# Note: In a real Databricks environment, you would use:\n",
    "# from databricks.sdk import WorkspaceClient\n",
    "# from pyspark.errors import PySparkException\n",
    "# w = WorkspaceClient()\n",
    "# w.secrets.list_scopes()\n",
    "\n",
    "# For local testing, we'll just use the webhook URL directly in the configuration\n",
    "print(\"Running in local Spark environment - using webhook URL directly\")\n",
    "webhook_url = \"https://hooks.slack.com/services/YOUR_WORKSPACE/YOUR_CHANNEL/YOUR_SECRET_TOKEN\"  # Replace with your actual webhook URL\n",
    "print(f\"Webhook URL configured: {webhook_url[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45040e9",
   "metadata": {},
   "source": [
    "# Widget Setup\n",
    "\n",
    "Widgets used in this notebook will be created and then set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78fba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856bce1ac84c4b2a8c9107bec601aeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='user', description='user: ', placeholder='Type something', style=TextStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GENERATE INPUT WIDGETS \n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "widget_user = widgets.Text(\n",
    "    value='user',\n",
    "    placeholder='Type something',\n",
    "    description='user: ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_git_org = widgets.Text(\n",
    "    value='catalog_name',\n",
    "    placeholder='Type something',\n",
    "    description='git_org ',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_catalog = widgets.Text(\n",
    "    value='development',\n",
    "    placeholder='Type something',\n",
    "    description='catalog:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}    \n",
    ")\n",
    "\n",
    "widget_schema = widgets.Text(\n",
    "    value='team_name',\n",
    "    placeholder='Type something',\n",
    "    description='schema:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_library_source = widgets.Combobox(\n",
    "    placeholder='Choose source',\n",
    "    options=['pypi', 'git'],\n",
    "    description='library_source:',\n",
    "    ensure_option=True,\n",
    "    value='git',\n",
    "    disabled=False,\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "widget_git_branch_or_commit = widgets.Text(\n",
    "    value='main',\n",
    "    placeholder='Type branch name or commit hash',\n",
    "    description='git_branch_or_commit:',\n",
    "    disabled=False,\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "widget_override_version = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Override SE version',\n",
    "    disabled=False,\n",
    "    style={'description_width': '30px'}\n",
    "    \n",
    ")\n",
    "\n",
    "hbox = widgets.HBox([\n",
    "    widget_user,\n",
    "    widget_catalog, \n",
    "    widget_schema,\n",
    "    widget_override_version, \n",
    "    widget_library_source, \n",
    "    widget_git_org,\n",
    "    widget_git_branch_or_commit\n",
    "])\n",
    "\n",
    "# Display widgets\n",
    "display(hbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "user = re.sub(r'[^a-zA-Z]', '', widget_user.value).lower()\n",
    "catalog = widget_catalog.value\n",
    "schema = widget_schema.value\n",
    "override_se_version = widget_override_version.value\n",
    "library = widget_library_source.value\n",
    "org = widget_git_org.value\n",
    "branch_or_commit = widget_git_branch_or_commit.value\n",
    "\n",
    "CONFIG = {\n",
    "    \"owner\": user,\n",
    "    \"catalog\": \"development\",\n",
    "    \"schema\": schema,\n",
    "    \"user\": user,\n",
    "    \"product_id\": f\"se_{user}_product\",\n",
    "    \"in_memory_source\": f\"se_{user}_source\",\n",
    "    \"rules_table\": f\"development.{schema}.se_{user}_rules\",\n",
    "    \"stats_table\": f\"development.{schema}.se_{user}_stats\",\n",
    "    \"target_table\": f\"development.{schema}.se_{user}_target\",\n",
    "    \"override_se_version\" : override_se_version,\n",
    "    \"library\": library,\n",
    "    \"org\": org,\n",
    "    \"branch_or_commit\": branch_or_commit\n",
    "}\n",
    "\n",
    "config_df = pd.DataFrame(list(CONFIG.items()), columns=['Key', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff7e12",
   "metadata": {},
   "source": [
    "# Install Spark Expectation\n",
    "\n",
    "If Running from local container it will come with latest spark-expectation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Using SparkExpectation from local codebase\n"
     ]
    }
   ],
   "source": [
    "# Override Spark Expectations based on user input\n",
    "if override_se_version:\n",
    "    print(\"-----OVERRIDING SPARK-EXPECTATIONS VERSION\")\n",
    "    if CONFIG[\"library\"] == \"pypi\":\n",
    "      print(\"-----INSTALLING SPARK-EXPECTATIONS from PyPi\")\n",
    "      %pip install spark-expectations\n",
    "    elif CONFIG[\"library\"] == \"git\":\n",
    "      print(f\"-----INSTALLING SPARK-EXPECTATIONS from Git Org/User {CONFIG['org']}, Branch/Commit {CONFIG['branch_or_commit']}\")\n",
    "      giturl = f\"git+https://github.com/{CONFIG['org']}/spark-expectations.git@{CONFIG['branch_or_commit']}\"\n",
    "      %pip install --force-reinstall {giturl}    \n",
    "else:\n",
    "    print(f\"---- Using SparkExpectation from local codebase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff84b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SPARK SESSION AND DATABASE\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Example\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd9946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n",
      "+---------+-------------+-----------+\n",
      "|namespace|tableName    |isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|default  |se_user_rules|false      |\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "databases_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases_df.show(truncate=False)\n",
    "\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6771bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 tables to drop.\n",
      "Dropped table: se_user_rules\n"
     ]
    }
   ],
   "source": [
    "db_name = f\"{CONFIG['catalog']}.{CONFIG['schema']}\"\n",
    "pattern = f\"se_{CONFIG['user']}*\"\n",
    "\n",
    "# Set the current catalog\n",
    "spark.sql(f\"USE {CONFIG['catalog']}.{CONFIG['schema']}\")\n",
    "\n",
    "# Drop tables matching pattern\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {db_name} LIKE '{pattern}'\")\n",
    "tables_to_drop = [row for row in tables_df.collect() if not row[\"isTemporary\"] ]\n",
    "\n",
    "if tables_to_drop:\n",
    "    print(f\"Found {len(tables_to_drop)} tables to drop.\")\n",
    "    for row in tables_to_drop:\n",
    "        table_name = row[\"tableName\"]\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{table_name}\")\n",
    "        print(f\"Dropped table: {db_name}.{table_name}\")\n",
    "else:\n",
    "    print(\"----- No tables to drop\")\n",
    "\n",
    "# Drop global and local temp views matching pattern\n",
    "\n",
    "views_df = spark.sql(f\"SHOW VIEWS in {db_name} LIKE '{pattern}'\")\n",
    "views_to_drop = views_df.collect()\n",
    "\n",
    "if views_to_drop:\n",
    "    print(f\"Found {len(views_to_drop)} views to drop.\")\n",
    "    for row in views_to_drop:\n",
    "        view_name = row[\"viewName\"]\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view_name}\")\n",
    "        print(f\"Dropped view: {view_name}\")\n",
    "else:\n",
    "    print(\"----- No views to drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd98b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------+--------------+-----------+-----------------+----------------+------------+------------------------+-------------------------------+-------------------------------+---------+-----------------------+--------------------+--------+\n",
      "|product_id     |table_name    |rule_type|rule          |column_name|expectation      |action_if_failed|tag         |description             |enable_for_source_dq_validation|enable_for_target_dq_validation|is_active|enable_error_drop_alert|error_drop_threshold|priority|\n",
      "+---------------+--------------+---------+--------------+-----------+-----------------+----------------+------------+------------------------+-------------------------------+-------------------------------+---------+-----------------------+--------------------+--------+\n",
      "|se_user_product|se_user_target|row_dq   |age_not_null  |age        |age IS NOT NULL  |warn            |completeness|Age must not be null    |true                           |true                           |true     |false                  |0                   |medium  |\n",
      "|se_user_product|se_user_target|row_dq   |age_adult     |age        |age < 20         |ignore          |validity    |Age must be less than 20|true                           |true                           |true     |false                  |0                   |medium  |\n",
      "|se_user_product|se_user_target|row_dq   |email_not_null|email      |email IS NOT NULL|drop            |completeness|Email must not be null  |true                           |true                           |true     |false                  |0                   |medium  |\n",
      "+---------------+--------------+---------+--------------+-----------+-----------------+----------------+------------+------------------------+-------------------------------+-------------------------------+---------+-----------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting Started with Spark Expectations: Simple Example\n",
    "\n",
    "## 1. Sample Source Dataset\n",
    "# initialize simple Pandas DataFrame and convert it to a Spark DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "## 2. Define Simple `row_dq` Rules\n",
    "# Create a rules DataFrame with a few simple data quality rules\n",
    "\n",
    "rules_data = [\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_not_null\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age IS NOT NULL\",\n",
    "        \"action_if_failed\": \"warn\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Age must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"age_adult\",\n",
    "        \"column_name\": \"age\",\n",
    "        \"expectation\": \"age < 20\",\n",
    "        \"action_if_failed\": \"ignore\",\n",
    "        \"tag\": \"validity\",\n",
    "        \"description\": \"Age must be less than 20\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    },\n",
    "        {\n",
    "        \"product_id\": CONFIG[\"product_id\"],\n",
    "        \"table_name\": CONFIG[\"target_table\"],\n",
    "        \"rule_type\": \"row_dq\",\n",
    "        \"rule\": \"email_not_null\",\n",
    "        \"column_name\": \"email\",\n",
    "        \"expectation\": \"email IS NOT NULL\",\n",
    "        \"action_if_failed\": \"drop\",\n",
    "        \"tag\": \"completeness\",\n",
    "        \"description\": \"Email must not be null\",\n",
    "        \"enable_for_source_dq_validation\": True,\n",
    "        \"enable_for_target_dq_validation\": True,\n",
    "        \"is_active\": True,\n",
    "        \"enable_error_drop_alert\": False,\n",
    "        \"error_drop_threshold\": 0,\n",
    "        \"priority\": \"medium\",\n",
    "    }\n",
    "\n",
    "    \n",
    "]\n",
    "rules_df = spark.createDataFrame(pd.DataFrame(rules_data))\n",
    "rules_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CONFIG['rules_table'])\n",
    "\n",
    "rules_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2cb182",
   "metadata": {},
   "source": [
    "# Notification Configurations for Slack\n",
    "\n",
    "This is where you can set the webhook URL with the following ways:\n",
    "- setting the webhook URL manually\n",
    "- using databricks secret storage (assuming you have secrets stored within your notebook)\n",
    "- using cerberus secrets storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccabfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure streaming and notification configuration\n",
    "from spark_expectations.config.user_config import Constants as user_config\n",
    "\n",
    "# This is a dictionary that can be used to configure Spark Expectations behavior and override default settings.\n",
    "stats_streaming_config_dict = {\n",
    "    user_config.se_enable_streaming: False,\n",
    "}\n",
    "\n",
    "# For local Spark environment, we don't need Databricks workspace info\n",
    "# dbx_workspace_id = get_context().workspaceId\n",
    "# dbx_workspace_url = get_context().browserHostName\n",
    "\n",
    "user_conf_dict = {\n",
    "    user_config.se_notifications_enable_slack: True,\n",
    "    # Slack Configuration - where you supply the webhook URL for your Slack channel\n",
    "    user_config.se_notifications_slack_webhook_url: \"your_webhook_url\",  # Replace with your actual webhook URL\n",
    "\n",
    "    # Fill in your cbs_url + cbs_sdb_path on where your secret is stored.\n",
    "    # user_config.secret_type: \"cerberus\",\n",
    "    # user_config.cbs_url: \"https://cerberus.com\",\n",
    "    # user_config.cbs_sdb_path: \"app/your/sdb/path\",\n",
    "\n",
    "    # Optionally configure additional Slack settings\n",
    "    # user_config.se_notifications_slack_channel: \"#data-quality-alerts\",\n",
    "    # user_config.se_notifications_slack_username: \"Spark Expectations Bot\",\n",
    "    # user_config.se_notifications_slack_icon_emoji: \":warning:\",\n",
    "\n",
    "    # Enable detailed results in notifications\n",
    "    user_config.se_enable_query_dq_detailed_result: True,\n",
    "    \n",
    "    # Set notification on query failure - corrected attribute name\n",
    "    user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n",
    "    user_config.se_notifications_on_error_drop_threshold: 15\n",
    "     # Notification triggers\n",
    "    user_config.se_notifications_on_start: True,\n",
    "    user_config.se_notifications_on_completion: True,\n",
    "    user_config.se_notifications_on_fail: True,\n",
    "    user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n",
    "    user_config.se_notifications_on_rules_action_if_failed_set_ignore: True,\n",
    "    user_config.se_notifications_on_error_drop_threshold: 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6a95a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 15:05:06 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------------+\n",
      "|id |age |email              |\n",
      "+---+----+-------------------+\n",
      "|1  |19.0|alice@example.com  |\n",
      "|2  |17.0|bob@example.com    |\n",
      "|3  |NaN |charlie@example.com|\n",
      "|4  |40.0|mike@example.com   |\n",
      "|5  |NaN |ron@example.com    |\n",
      "|6  |35.0|NULL               |\n",
      "+---+----+-------------------+\n",
      "\n",
      "[2025-10-17 15:05:09,660] [INFO] [spark_expectations] {slack.py:send_notification:42} - Message posted successfully!\n",
      "[2025-10-17 15:05:09,664] [INFO] [spark_expectations] {expectations.py:wrapper:320} - The function dataframe is getting created\n",
      "[2025-10-17 15:05:09,660] [INFO] [spark_expectations] {slack.py:send_notification:42} - Message posted successfully!\n",
      "[2025-10-17 15:05:09,664] [INFO] [spark_expectations] {expectations.py:wrapper:320} - The function dataframe is getting created\n",
      "[2025-10-17 15:05:09,933] [INFO] [spark_expectations] {expectations.py:wrapper:336} - Validation for rules completed successfully\n",
      "[2025-10-17 15:05:09,933] [INFO] [spark_expectations] {expectations.py:wrapper:336} - Validation for rules completed successfully\n",
      "[2025-10-17 15:05:10,290] [INFO] [spark_expectations] {expectations.py:wrapper:339} - data frame input record count: 6\n",
      "[2025-10-17 15:05:10,290] [INFO] [spark_expectations] {expectations.py:wrapper:351} - initialize variable with default values before next run\n",
      "[2025-10-17 15:05:10,291] [INFO] [spark_expectations] {expectations.py:wrapper:385} - Spark Expectations run id for this run: se_user_product_56fba768-aba5-11f0-91bd-de847171f901\n",
      "[2025-10-17 15:05:10,291] [INFO] [spark_expectations] {expectations.py:wrapper:388} - The function dataframe is created\n",
      "[2025-10-17 15:05:10,291] [INFO] [spark_expectations] {expectations.py:wrapper:391} - Dropping to temp table started\n",
      "[2025-10-17 15:05:10,290] [INFO] [spark_expectations] {expectations.py:wrapper:339} - data frame input record count: 6\n",
      "[2025-10-17 15:05:10,290] [INFO] [spark_expectations] {expectations.py:wrapper:351} - initialize variable with default values before next run\n",
      "[2025-10-17 15:05:10,291] [INFO] [spark_expectations] {expectations.py:wrapper:385} - Spark Expectations run id for this run: se_user_product_56fba768-aba5-11f0-91bd-de847171f901\n",
      "[2025-10-17 15:05:10,291] [INFO] [spark_expectations] {expectations.py:wrapper:388} - The function dataframe is created\n",
      "[2025-10-17 15:05:10,291] [INFO] [spark_expectations] {expectations.py:wrapper:391} - Dropping to temp table started\n",
      "[2025-10-17 15:05:10,320] [INFO] [spark_expectations] {expectations.py:wrapper:393} - Dropping to temp table completed\n",
      "[2025-10-17 15:05:10,320] [INFO] [spark_expectations] {expectations.py:wrapper:394} - Writing to temp table started\n",
      "[2025-10-17 15:05:10,320] [INFO] [spark_expectations] {expectations.py:wrapper:393} - Dropping to temp table completed\n",
      "[2025-10-17 15:05:10,320] [INFO] [spark_expectations] {expectations.py:wrapper:394} - Writing to temp table started\n",
      "[2025-10-17 15:05:10,340] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:10,342] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_target_temp\n",
      "[2025-10-17 15:05:10,340] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:10,342] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_target_temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 268:====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:14,952] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: se_user_target_temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:15,215] [INFO] [spark_expectations] {writer.py:save_df_as_table:99} - product_id is not set for table se_user_target_temp in tableproperties, setting it now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 275:====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:19,386] [INFO] [spark_expectations] {expectations.py:wrapper:401} - Read from temp table started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:19,642] [INFO] [spark_expectations] {expectations.py:wrapper:404} - Read from temp table completed\n",
      "[2025-10-17 15:05:19,643] [INFO] [spark_expectations] {expectations.py:wrapper:476} - started processing data quality rules for row level expectations\n",
      "[2025-10-17 15:05:19,643] [INFO] [spark_expectations] {regulate_flow.py:func_process:92} - The data quality dataframe is getting created for expectations\n",
      "[2025-10-17 15:05:19,643] [INFO] [spark_expectations] {expectations.py:wrapper:476} - started processing data quality rules for row level expectations\n",
      "[2025-10-17 15:05:19,643] [INFO] [spark_expectations] {regulate_flow.py:func_process:92} - The data quality dataframe is getting created for expectations\n",
      "[2025-10-17 15:05:19,883] [INFO] [spark_expectations] {regulate_flow.py:func_process:103} - The data quality dataframe is created for expectations\n",
      "[2025-10-17 15:05:19,884] [INFO] [spark_expectations] {regulate_flow.py:func_process:113} - Writing error records into the table started\n",
      "[2025-10-17 15:05:19,885] [INFO] [spark_expectations] {writer.py:write_error_records_final:776} - _write_error_records_final started\n",
      "[2025-10-17 15:05:19,883] [INFO] [spark_expectations] {regulate_flow.py:func_process:103} - The data quality dataframe is created for expectations\n",
      "[2025-10-17 15:05:19,884] [INFO] [spark_expectations] {regulate_flow.py:func_process:113} - Writing error records into the table started\n",
      "[2025-10-17 15:05:19,885] [INFO] [spark_expectations] {writer.py:write_error_records_final:776} - _write_error_records_final started\n",
      "self._context.get_se_enable_error_table : True\n",
      "[2025-10-17 15:05:20,057] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:20,061] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_target_error\n",
      "self._context.get_se_enable_error_table : True\n",
      "[2025-10-17 15:05:20,057] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:20,061] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_target_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 287:====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:29,595] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: se_user_target_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:33,844] [INFO] [spark_expectations] {writer.py:write_error_records_final:828} - _write_error_records_final ended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:38,918] [INFO] [spark_expectations] {slack.py:send_notification:42} - Message posted successfully!\n",
      "[2025-10-17 15:05:38,920] [INFO] [spark_expectations] {expectations.py:wrapper:524} - ended processing data quality rules for row level expectations\n",
      "[2025-10-17 15:05:38,920] [INFO] [spark_expectations] {expectations.py:wrapper:524} - ended processing data quality rules for row level expectations\n",
      "[2025-10-17 15:05:39,279] [INFO] [spark_expectations] {slack.py:send_notification:42} - Message posted successfully!\n",
      "[2025-10-17 15:05:39,281] [INFO] [spark_expectations] {expectations.py:wrapper:617} - Writing into the final table started\n",
      "[2025-10-17 15:05:39,279] [INFO] [spark_expectations] {slack.py:send_notification:42} - Message posted successfully!\n",
      "[2025-10-17 15:05:39,281] [INFO] [spark_expectations] {expectations.py:wrapper:617} - Writing into the final table started\n",
      "[2025-10-17 15:05:39,459] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:39,463] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_target\n",
      "[2025-10-17 15:05:39,459] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:39,463] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 319:==========================================>            (39 + 8) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:48,124] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: se_user_target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:48,472] [INFO] [spark_expectations] {expectations.py:wrapper:623} - Writing into the final table ended\n",
      "[2025-10-17 15:05:49,051] [INFO] [spark_expectations] {writer.py:write_error_stats:732} - Writing metrics to the stats table: {self._context.get_dq_stats_table_name}, started\n",
      "[2025-10-17 15:05:49,051] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:49,062] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_stats\n",
      "[2025-10-17 15:05:49,051] [INFO] [spark_expectations] {writer.py:write_error_stats:732} - Writing metrics to the stats table: {self._context.get_dq_stats_table_name}, started\n",
      "[2025-10-17 15:05:49,051] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:49,062] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 330:===================================================>   (47 + 3) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:05:54,050] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: se_user_stats\n",
      "[2025-10-17 15:05:54,051] [INFO] [spark_expectations] {writer.py:write_error_stats:743} - Writing metrics to the stats table: {self._context.get_dq_stats_table_name}, ended\n",
      "[2025-10-17 15:05:54,051] [INFO] [spark_expectations] {writer.py:write_error_stats:743} - Writing metrics to the stats table: {self._context.get_dq_stats_table_name}, ended\n",
      "[2025-10-17 15:05:54,200] [INFO] [spark_expectations] {writer.py:write_detailed_stats:487} - Writing metrics to the detailed stats table: {self._context.get_dq_detailed_stats_table_name}, started\n",
      "[2025-10-17 15:05:54,201] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:54,204] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_stats_detailed\n",
      "[2025-10-17 15:05:54,200] [INFO] [spark_expectations] {writer.py:write_detailed_stats:487} - Writing metrics to the detailed stats table: {self._context.get_dq_detailed_stats_table_name}, started\n",
      "[2025-10-17 15:05:54,201] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:05:54,204] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_stats_detailed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 343:====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:06:00,968] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: se_user_stats_detailed\n",
      "[2025-10-17 15:06:00,969] [INFO] [spark_expectations] {writer.py:write_detailed_stats:498} - Writing metrics to the detailed stats table: se_user_stats_detailed, ended\n",
      "[2025-10-17 15:06:00,969] [INFO] [spark_expectations] {writer.py:write_detailed_stats:498} - Writing metrics to the detailed stats table: se_user_stats_detailed, ended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:06:01,345] [INFO] [spark_expectations] {writer.py:write_detailed_stats:505} - Writing metrics to the output custom table: {self._context.get_query_dq_output_custom_table_name}, started\n",
      "[2025-10-17 15:06:01,346] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:06:01,349] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_stats_querydq_output\n",
      "[2025-10-17 15:06:01,346] [INFO] [spark_expectations] {writer.py:save_df_as_table:64} - _save_df_as_table started\n",
      "[2025-10-17 15:06:01,349] [INFO] [spark_expectations] {writer.py:save_df_as_table:82} - Writing records to table: se_user_stats_querydq_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 355:====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:06:07,028] [INFO] [spark_expectations] {writer.py:save_df_as_table:88} - finished writing records to table: se_user_stats_querydq_output\n",
      "[2025-10-17 15:06:07,028] [INFO] [spark_expectations] {writer.py:write_detailed_stats:516} - Writing metrics to the output custom table: {self._context.get_query_dq_output_custom_table_name}, ended\n",
      "[2025-10-17 15:06:07,029] [INFO] [spark_expectations] {writer.py:write_error_stats:769} - Streaming stats to kafka is disabled, hence skipping writing to kafka\n",
      "[2025-10-17 15:06:07,028] [INFO] [spark_expectations] {writer.py:write_detailed_stats:516} - Writing metrics to the output custom table: {self._context.get_query_dq_output_custom_table_name}, ended\n",
      "[2025-10-17 15:06:07,029] [INFO] [spark_expectations] {writer.py:write_error_stats:769} - Streaming stats to kafka is disabled, hence skipping writing to kafka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 15:06:07,370] [INFO] [spark_expectations] {slack.py:send_notification:42} - Message posted successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, age: double, email: string, meta_dq_run_id: string, meta_dq_run_datetime: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3. Run Spark Expectations\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from spark_expectations.core import load_configurations\n",
    "\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "\n",
    "\n",
    "writer = WrappedDataFrameWriter().mode(\"overwrite\").format(\"delta\")\n",
    "\n",
    "\n",
    "# Initialize Default Config \n",
    "load_configurations(spark) \n",
    "\n",
    "\"\"\"\n",
    "This class implements/supports running the data quality rules on a dataframe returned by a function\n",
    "\n",
    "Args:\n",
    "    product_id: Name of the product\n",
    "    rules_df: DataFrame which contains the rules. User is responsible for reading\n",
    "        the rules_table in which ever system it is\n",
    "    stats_table: Name of the table where the stats/audit-info need to be written\n",
    "    debugger: Mark it as \"True\" if the debugger mode need to be enabled, by default is False\n",
    "    stats_streaming_options: Provide options to override the defaults, while writing into the stats streaming table\n",
    "\"\"\"\n",
    "se = SparkExpectations(\n",
    "    product_id=CONFIG[\"product_id\"],\n",
    "    rules_df=rules_df,\n",
    "    stats_table=CONFIG[\"stats_table\"],\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    stats_streaming_options=stats_streaming_config_dict,\n",
    ")\n",
    "\n",
    "#  Initialize input data\n",
    "data = [\n",
    "    {\"id\": 1, \"age\": 19,   \"email\": \"alice@example.com\"},\n",
    "    {\"id\": 2, \"age\": 17,   \"email\": \"bob@example.com\"},\n",
    "    {\"id\": 3, \"age\": None, \"email\": \"charlie@example.com\"},\n",
    "    {\"id\": 4, \"age\": 40,   \"email\": \"mike@example.com\"},\n",
    "    {\"id\": 5, \"age\": None, \"email\": \"ron@example.com\"},\n",
    "    {\"id\": 6, \"age\": 35,   \"email\": None},\n",
    "]\n",
    "input_df = spark.createDataFrame(pd.DataFrame(data))\n",
    "input_df.show(truncate=False)\n",
    "\n",
    "\"\"\"\n",
    "This decorator helps to wrap a function which returns dataframe and apply dataframe rules on it\n",
    "\n",
    "Args:\n",
    "    target_table: Name of the table where the final dataframe need to be written\n",
    "    write_to_table: Mark it as \"True\" if the dataframe need to be written as table\n",
    "    write_to_temp_table: Mark it as \"True\" if the input dataframe need to be written to the temp table to break\n",
    "                        the spark plan\n",
    "    user_conf: Provide options to override the defaults, while writing into the stats streaming table\n",
    "    target_table_view: This view is created after the _row_dq process to run the target agg_dq and query_dq.\n",
    "        If value is not provided, defaulted to {target_table}_view\n",
    "    target_and_error_table_writer: Provide the writer to write the target and error table,\n",
    "        this will take precedence over the class level writer\n",
    "\n",
    "Returns:\n",
    "    Any: Returns a function which applied the expectations on dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@se.with_expectations(\n",
    "    target_table=CONFIG[\"target_table\"],\n",
    "    write_to_table=True,\n",
    "    write_to_temp_table=True,\n",
    "    user_conf=user_conf_dict,\n",
    ")\n",
    "def get_dataset():\n",
    "    _df_source: DataFrame = input_df\n",
    "    _df_source.createOrReplaceTempView(CONFIG[\"in_memory_source\"])\n",
    "    return _df_source\n",
    "\n",
    "\n",
    "# This will run the DQ checks and raise if any \"fail\" rules are violated\n",
    "get_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Project Env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
